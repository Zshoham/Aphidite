@misc{YOLO,
  doi       = {10.48550/ARXIV.1506.02640},
  url       = {https://arxiv.org/abs/1506.02640},
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{FasterRCNN,
  author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  url       = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
  volume    = {28},
  year      = {2015}
}

@article{AutoSegmentation,
  title     = {Automatic Segmentation and Counting of Aphid Nymphs on Leaves Using Convolutional Neural Networks},
  volume    = {8},
  issn      = {2073-4395},
  url       = {http://dx.doi.org/10.3390/agronomy8080129},
  doi       = {10.3390/agronomy8080129},
  number    = {8},
  journal   = {Agronomy},
  publisher = {MDPI AG},
  author    = {Chen, Jian and Fan, Yangyang and Wang, Tao and Zhang, Chu and Qiu, Zhengjun and He, Yong},
  year      = {2018},
  month     = {Jul},
  pages     = {129}
}

@article{DetectionInWeatFields,
  title    = {Detection of aphids in wheat fields using a computer vision technique},
  journal  = {Biosystems Engineering},
  volume   = {141},
  pages    = {82-93},
  year     = {2016},
  issn     = {1537-5110},
  doi      = {https://doi.org/10.1016/j.biosystemseng.2015.11.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S1537511015300866},
  author   = {Tao Liu and Wen Chen and Wei Wu and Chengming Sun and Wenshan Guo and Xinkai Zhu},
  keywords = {Computer vision, Histogram of oriented gradient (HOG) feature, Identification and counting, Maximally stable extremal region (MSER) detection, Wheat aphid},
  abstract = {Aphids cause major damage in wheat fields resulting in significant yield losses. Monitoring aphid populations and the identification of aphid species provides important data related to pest population dynamics and integrated pest management. Manual identification and counting of wheat aphids is labour intensive, inefficient and subjective factors can influence its accuracy. A method of aphid identification and population monitoring based on digital images was developed. It used a maximally stable extremal region descriptor to simplify the background of field images containing aphids, and then used histograms of oriented gradient features and a support vector machine to develop an aphid identification model. This method was compared with five other commonly used methods of aphid detection; their performance was analysed using images with different aphid density, colour, or location on the plant. The results demonstrated that our new method provided mean identification and error rates of 86.81% and 8.91%, respectively, which is superior to other methods. The proposed method was easy-to-use and provides efficient and accurate aphid population data, and therefore can be used for aphid infestation surveys in wheat fields.}
}

@article{CoarseToFine,
  title    = {A coarse-to-fine network for aphid recognition and detection in the field},
  journal  = {Biosystems Engineering},
  volume   = {187},
  pages    = {39-52},
  year     = {2019},
  issn     = {1537-5110},
  doi      = {https://doi.org/10.1016/j.biosystemseng.2019.08.013},
  url      = {https://www.sciencedirect.com/science/article/pii/S1537511019308086},
  author   = {Rui Li and Rujing Wang and Chengjun Xie and Liu Liu and Jie Zhang and Fangyuan Wang and Wancai Liu},
  keywords = {Aphid Detection, Aphid Recognition, Convolutional Neural Network, Coarse-to-Fine Network},
  abstract = {In agriculture, aphids are one of the most destructive pests, responsible for major reductions in wheat, corn and rape production leading to significant economic losses. However, manual pest recognition approaches are often time-consuming and laborious for Integrated Pest Management (IPM). In addition, the existing pest detection methods based on Convolutional Neural Network (CNN) are not satisfactory for small aphid recognition and detection in the field because aphids are tiny and often in dense distributions. In this work, a two-stage aphid detector named Coarse-to-Fine Network (CFN) is proposed to address these problems. The key idea of our method is to develop a Coarse Convolutional Neural Network (CCNN) for aphid clique searching as well as a Fine Convolutional Neural Network (FCNN) for refining the regions of aphids in the clique. Specifically, The CCNN detects approximately all the object regions from natural aphid images with various aphid distributions including dense aphid cliques and sparse aphid objects, in which an Improved Non-Maximum Suppression (INMS) strategy is proposed to eliminate overlapping regions. Then, the FCNN further refines the detected aphid regions from the CCNN. The final recognition and detection result would be obtained by combining the outputs from CCNN and FCNN together. Experiments on our dataset show that our CFN achieves an aphid detection performance of 76.8% Average Precision (AP), which improves 20.9%, 18%,13.7% and 12.5% compared to four state-of-the-art approaches.}
}

@article{TDDET,
  title     = {TD-Det: A Tiny Size Dense Aphid Detection Network under In-Field Environment},
  volume    = {13},
  issn      = {2075-4450},
  url       = {http://dx.doi.org/10.3390/insects13060501},
  doi       = {10.3390/insects13060501},
  number    = {6},
  journal   = {Insects},
  publisher = {MDPI AG},
  author    = {Teng, Yue and Wang, Rujing and Du, Jianming and Huang, Ziliang and Zhou, Qiong and Jiao, Lin},
  year      = {2022},
  month     = {May},
  pages     = {501}
}

@INPROCEEDINGS{PestYOLO,
  author={Tang, Zhe and Chen, Zhengyun and Qi, Fang and Zhang, Lingyan and Chen, Shuhong},
  booktitle={2021 IEEE International Conference on Data Mining (ICDM)}, 
  title={Pest-YOLO: Deep Image Mining and Multi-Feature Fusion for Real-Time Agriculture Pest Detection}, 
  year={2021},
  volume={},
  number={},
  pages={1348-1353},
  doi={10.1109/ICDM51629.2021.00169}
}

@article{DenseTinyPest,
title = {Towards densely clustered tiny pest detection in the wild environment},
journal = {Neurocomputing},
volume = {490},
pages = {400-412},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018506},
author = {Jianming Du and Liu Liu and Rui Li and Lin Jiao and Chengjun Xie and Rujing Wang},
keywords = {Pest detection, Clustered tiny object, Small object detection, Image dataset},
abstract = {Our life is populated with many small-size objects, such as human in aerial images and tiny pests in agriculture. Current generic and small object detection methods are only focus on tackling their sizes rather than distribution. Considering this limitation, we state a Densely Clustered Tiny (DCT) object detection problem using a novel metric Object Density Level (ODL) to measure the object distribution in an image. The DCT problem allows varied densely distributed objects in the real-world captured images. In dealing with the DCT problem, we select two kinds of aphids that usually gather into cliques in the real-world agricultural environment, and build an aphid dataset APHID-4K in our task. Accompanying the DCT task, we propose a novel DCT detection network (DCTDet) to address this challenge. Specifically, a Cluster Region Proposal Network (ClusRPN) is trained to select appropriate densely distributed object cluster regions from images. These candidates are classified into different groups according to their density. A Density Merging and Partition module (DMP) merges and partitions them respectively and finally outputs cluster regions with uniform size and density to a subsequent Local Detector Group (LDG). In addition, we also use Composited Cluster data Generation (CCG) to present a large-scale dataset for ClusRPN optimization for robust training procedure and theoretically analyze their effects in detail. Experiments on APHID-4K and another clustered small object detection dataset VisDrone show that our DCTDet achieves state-of-the-art performance.}
}

@article{MultiBranchCNN,
title = {A multi-branch convolutional neural network with density map for aphid counting},
journal = {Biosystems Engineering},
volume = {213},
pages = {148-161},
year = {2022},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2021.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1537511021002877},
author = {Rui Li and Rujing Wang and Chengjun Xie and Hongbo Chen and Qi Long and Liu Liu and Jie Zhang and Tianjiao Chen and Haiying Hu and Lin Jiao and Jianming Du and Haiyun Liu},
keywords = {Density map, Aphid counting, Convolutional neural network, Deep learning},
abstract = {In agriculture, aphids always cause major damage in wheat, corn and rape, which significantly affect the crop yield. Manual aphid counting approaches are often labour-consuming and time-costing for Integrated Pest Management (IPM). In addition, the results of existing aphid counting methods based on computer vision are not satisfactory due to the complex background and the dense distribution. In order to address these problems, a novel multi-branch convolutional neural network (Mb-CNN) with density map for aphid counting is developed in this paper. In this approach, the aphid images are firstly fed into multi-branch convolutional neural networks, which have three branches for extracting the feature maps of different scales. Then, an aphid density map is generated via Mb-CNN, which contains the distribution information of aphids. Finally, the counting of aphids is estimated by using the density map. Experiment results on our dataset demonstrate that our Mb-CNN achieves the performance of 10.22 Mean Absolute Error (MAE) and 12.24 Mean Squared Error (MSE) in the aphid counting, which outweighs the state-of-the-art approaches.}
}

@misc{HRDNet,
  doi = {10.48550/ARXIV.2006.07607},
  
  url = {https://arxiv.org/abs/2006.07607},
  
  author = {Liu, Ziming and Gao, Guangyu and Sun, Lin and Fang, Zhiyuan},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {HRDNet: High-resolution Detection Network for Small Objects},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{AugSmallObject,
  doi = {10.48550/ARXIV.1902.07296},
  
  url = {https://arxiv.org/abs/1902.07296},
  
  author = {Kisantal, Mate and Wojna, Zbigniew and Murawski, Jakub and Naruniec, Jacek and Cho, Kyunghyun},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Augmentation for small object detection},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{BioBasedClassical,
title = {Research on insect pest image detection and recognition based on bio-inspired methods},
journal = {Biosystems Engineering},
volume = {169},
pages = {139-148},
year = {2018},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2018.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1537511017302969},
author = {Limiao Deng and Yanjiang Wang and Zhongzhi Han and Renshi Yu},
keywords = {Pest recognition, Invariant features, HMAX model, Saliency map, Bio-inspired},
abstract = {Insect pest recognition and detection are vital for food security, a stable agricultural economy and quality of life. To realise rapid detection and recognition of insect pests, methods inspired by human visual system were proposed in this paper. Inspired by human visual attention, Saliency Using Natural statistics model (SUN) was used to generate saliency maps and detect region of interest (ROI) in a pest image. To extract the invariant features for representing the pest appearance, we extended the bio-inspired Hierarchical Model and X (HMAX) model in the following ways. Scale Invariant Feature Transform (SIFT) was integrated into the HMAX model to increase the invariance to rotational changes. Meanwhile, Non-negative Sparse Coding (NNSC) is used to simulate the simple cell responses. Moreover, invariant texture features were extracted based on Local Configuration Pattern (LCP) algorithm. Finally, the extracted features were fed to Support Vector Machines (SVM) for recognition. Experimental results demonstrated that the proposed method had an advantage over the compared methods: HMAX, Sparse Coding and Natural Input Memory with Bayesian Likelihood Estimation (NIMBLE), and was comparable to the Deep Convolutional Network. The proposed method has achieved a good result with a recognition rate of 85.5% and could effectively recognise insect pest under complex environments. The proposed method has provided a new approach for insect pest detection and recognition.}
}
